{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Pandemic - Part I"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notes: Tonight we are going to be concentrating on optimal placement of research stations to improve movement efficiency. In order to do this, we need something to measure and a way to measure it. There are a billion ways to do this, and my hope is that some of you go home, download my code from github, plug in a different metric than I used, and share your results back with me.  \n",
    "\n",
    "I chose to concentrate on the lengths of paths between pairs of cities and specifically the worst case scenario. What I mean is that I was concerned with minimizing the shortest distance between any two cities and measured this by looking for the longest of all the shortest paths between all pairs of cities on the graph. The longest shortest path of a graph is called the *diameter* of the graph. So I was trying to minimize the diameter of the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![board](images\\diameter_path.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next graphic shows a good research station placed, the new edge from atlanta to new RS, and the new shortest path between those cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![board](images\\bad_rs_placement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next graphic shows a good research station placed, the new edge from atlanta to new RS, and the new shortest path between those cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![board](images\\good_rs_placement.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notes: What else could we measure? Well, instead of looking at the worst case and finding the longest of the shortest paths, we could say we want to minimize the average shortest path, which might give better results. What if we want to look at something other than paths? We could somehow use the number of routes (edges) coming out of each city (node). The number of egdes connected to a node is called the *degree* of the node. There's a node characteristic called the *eigenvector centrality*[1] that looks at the degree of a node as well as the degrees of all the adjacent nodes and calculates how *central* that node is to the graph (see reference for an analysis using this metric).  \n",
    "\n",
    "Diameter is a good place to start because it's easy to understand, easy to calculate, and easy to interpret its impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See _Pandemic Analysis - Generate data.ipynb_    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin:500px\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "#import pandemic\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load Pandemic board as graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandemic_g = nx.read_graphml('pandemic.graphml.txt')\n",
    "\n",
    "# dicts from city names to numbers, and from numbers to city names.\n",
    "city_names_to_num = {tup[1]['label']:tup[0] for tup in pandemic_g.nodes(data=True)}\n",
    "city_num_to_names = {tup[0]:tup[1]['label'] for tup in pandemic_g.nodes(data=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculate starting diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.diameter(pandemic_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through all the json results files, load them into dataframe, rename the columns, \n",
    "# and concat them all together.\n",
    "df = pd.concat([pd.read_json('{}_nodes.json'.format(i)).\n",
    "                rename(columns={0:'Diameter', 1:'Stations', 2:'Num_Edges'})\n",
    "                for i in xrange(1,6)],\n",
    "                ignore_index=True).drop('Num_Edges', axis=1)\n",
    "# Add column for number of stations per combination.\n",
    "df['Num_Stations'] = df.Stations.map(lambda x: len(x))\n",
    "# Add column with names of staitons.\n",
    "df['City_Names'] = df.Stations.map(lambda stations: [city_num_to_names[s] for s in stations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute minimum diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.Diameter.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abs_min_dia = df[df.Diameter==5]\n",
    "abs_min_dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Number of station combos with min diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(abs_min_dia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Minimum number of stations needed to get min diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abs_min_dia.Num_Stations.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all of the minimum combos have six stations.  \n",
    "You cannot get a diameter of five with fewer stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin:500px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum diameter by number of stations in combo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is the minimum diameter for each number of stations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the minimum diameter by number of stations.\n",
    "pd.DataFrame(df.groupby('Num_Stations')['Diameter'].min())  # Convert to dataframe for easier reading."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notes: If you are placing an optimal combo with 4 research stations, you cannot improve the minimum diameter by placing a fifth station. This is were interpreting your metric (diameter) is important. Even though this table says that placing a fifth station has no benefit, obviously in the game placing a fifth station is going to help you out by shortening paths between some cities. This table is just telling us that you cannot improve the *worst case* by going from four to five stations. You would see a different result here if we were using a different metric such as the average shortest paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How many combos yield the minimum diameter by number of stations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the number of combos with the minimum diameter by number of stations.\n",
    "(pd.concat([df.groupby('Num_Stations')['Diameter'].min(),\n",
    "           df.groupby('Num_Stations').apply(lambda grp: len(grp[grp.Diameter==grp.Diameter.min()])), \n",
    "           df.groupby('Num_Stations').size()],\n",
    "         axis=1).rename(columns={'Diameter':'Min diameter', 0:'Combos with min diameter', 1:'Total combos'})\n",
    " [['Total combos', 'Combos with min diameter','Min diameter']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Filter for just the combinations that give the minimum diamater by number of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_dia_by_num_of_stations = df.groupby('Num_Stations').apply(lambda grp: grp[grp.Diameter==grp.Diameter.min()]).drop('Stations', axis=1)\n",
    "min_dia_by_num_of_stations.drop('Num_Stations', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Most common cities in minimum combos by number of stations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "most_common_in_min_combos_by_num_of_stats = (min_dia_by_num_of_stations.groupby('Num_Stations')\n",
    "                        .apply(lambda grp: Counter(itertools.chain(*grp.Stations))))\n",
    "\n",
    "most_common_in_min_combos_by_num_of_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_in_min_combos_by_num_of_stats = (min_dia_by_num_of_stations.groupby('Num_Stations')\n",
    "                        .apply(lambda grp: Counter(itertools.chain(*grp.City_Names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_in_min_combos_by_num_of_stats[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_in_min_combos_by_num_of_stats[3].most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see groups start to form of cities clustered together that represent the same area of the graph and probably appear in separate optimal combos:  \n",
    "Istanbul, Baghdad, Cairo  \n",
    "Hong Kong, Shanghai, Taipei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![board](images/3_city_groups.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_dia_by_num_of_stations[min_dia_by_num_of_stations.City_Names.map(lambda grp: u'S\\xe3o Paulo' in grp)].loc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![board](images/sao_paulo_3_combos.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_in_min_combos_by_num_of_stats[4].most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_in_min_combos_by_num_of_stats[5].most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_common_in_min_combos_by_num_of_stats[6].most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Which cities appear in the most combos across all groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, calculate the fractions within each number of stations.\n",
    "cities_frac = []\n",
    "for num_of_stations in xrange(2,7):\n",
    "    c = most_common_in_min_combos_by_num_of_stats[num_of_stations]\n",
    "    #cities = map(lambda tup: (pandemic.city_num_to_names[tup[0]], tup[1]), c.most_common(None))\n",
    "    cities = c.most_common(None)\n",
    "    total_combos = float(cities[0][1])\n",
    "    for city in cities:\n",
    "        cities_frac.append((city[0], city[1], city[1]/total_combos, num_of_stations))\n",
    "\n",
    "frac_df = pd.DataFrame.from_records(cities_frac, columns=['City', 'Occurrences', 'Fraction', 'Num_Stations'])\n",
    "frac_df = frac_df.set_index(['Num_Stations', 'City'])\n",
    "frac_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sum fractions for each city to get a metric we can use to rank the cities.\n",
    "rankings = (pd.DataFrame(frac_df.reset_index()\n",
    "                         .groupby('City')['Fraction']\n",
    "                         .sum()\n",
    "                         .sort_values(ascending=False))\n",
    "            .reset_index()).rename(columns={'Fraction':'Weighted Sum'})\n",
    "# Add the degree of each city.\n",
    "rankings['Degree'] = rankings.City.map(lambda c: pandemic_g.degree(city_names_to_num[c]))\n",
    "rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are summing fractions from separate groups, so the total is not a fraction but more like a weighted sum. The thought is that simply counting the number of optimal combos each city appears in could be biased toward cities that are part of combos with more stations (4, 5, 6 stations). Instead, we want to know the normalize frequency with which each city appears in an optimal combo for each group (number of stations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each city, how many optimal combos does it appear in for each number of stations.\n",
    "city_optimals_by_num_of_stations = frac_df.drop('Fraction', axis=1).unstack('Num_Stations')\n",
    "city_optimals_by_num_of_stations.columns = city_optimals_by_num_of_stations.columns.droplevel(0)\n",
    "city_optimals_by_num_of_stations = city_optimals_by_num_of_stations.fillna(0).astype(int)\n",
    "city_optimals_by_num_of_stations['Total'] = city_optimals_by_num_of_stations.sum(axis=1)\n",
    "# Merge the rankings with the above dataframe.\n",
    "rankings = rankings.merge(city_optimals_by_num_of_stations, how='left', left_on='City', right_index=True)\n",
    "rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Hong Kong and Cairo are about 2x as important as the next cities on the list, Baghdad and Mexico City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![board](images\\hk_cairo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin:500px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine which optimal combos are subsets of other optimal combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimal_subsets = min_dia_by_num_of_stations.reset_index(level=1,drop=True).assign(City_Names=lambda df: df.City_Names.map(frozenset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate combos that are subsets of larger combos.\n",
    "sub_of = {}\n",
    "for i in optimal_subsets.index.unique()[:-1]:\n",
    "    for sub_combo in optimal_subsets.City_Names.loc[i]:\n",
    "        sub_of[sub_combo] = [c for c in optimal_subsets.City_Names.loc[i+1] if sub_combo.issubset(c)]\n",
    "sub_of = {k:v for k,v in sub_of.iteritems() if len(v) > 0}                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter down to combos of less than 4 for easier viewing.\n",
    "{k:v for k,v in sub_of.items() if len(k) < 4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How many combinations are subsets of larger combinations by number of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Counter([len(s) for s in sub_of])\n",
    "pd.DataFrame(c.items(), columns=['Number of Stations', 'Number of combos that are subsets of a larger combo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted([sorted(list(s)) for s in sub_of if len(s) < 3], key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted([sorted(list(s)) for s in sub_of if len(s) < 4], key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build graph to find longest chain of subsetting optimal combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimal_combos_graph = nx.DiGraph()\n",
    "optimal_combos_graph.add_edges_from(itertools.chain.from_iterable([itertools.product([k], v) for k,v in sub_of.iteritems()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.descendants(optimal_combos_graph, frozenset({u'Atlanta', u'Cairo'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add 'sink' node to making calculating chains easier.\n",
    "for n in optimal_combos_graph.nodes():\n",
    "    if len(n) == 6:\n",
    "        optimal_combos_graph.add_edge(n, 'sink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "longest_combo_chains = list(nx.all_shortest_paths(optimal_combos_graph, frozenset({u'Atlanta', u'Cairo'}), 'sink'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(longest_combo_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chain = longest_combo_chains[0]\n",
    "print(list(chain[0]))\n",
    "for i in xrange(1, len(chain)-1):\n",
    "    print(list(chain[i] - chain[i-1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "longest_combo_chains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
